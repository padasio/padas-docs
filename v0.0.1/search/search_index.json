{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PADAS Overview PADAS is built on top of robust Confluent Kafka Platform and enables organizations to transform, analyze, and filter TBs of streaming data in real-time. The goal is to keep things simple and take some of the burden away from existing SIEM and Analytics platforms by running various pipelines (combination of tasks that perform transformations and detections). PADAS comes with out-of-the-box integration examples (e.g. Winlogbeat, Splunk) and ready-to-implement rules pertinent to MITRE ATT&CK Framework. Any other custom rules can also be added (via PDL) without any dependency to existing SIEM or Analytics infrastructure. Set up in 5 minutes Review PADAS design and architectural considerations and quickly get started on your preferred platform. Getting started It's just Markdown Details on system requirements and how to install and run PADAS in production environments. Reference Made to measure This guide provides how to utilize PADAS Manager for configuring properties and PDL rules. Customization Open Source, MIT Integrate with data ingest pipelines (e.g. Winlogbeat) and downstream SIEM/Analytics platforms (e.g. Splunk) License","title":"Overview"},{"location":"#padas-overview","text":"PADAS is built on top of robust Confluent Kafka Platform and enables organizations to transform, analyze, and filter TBs of streaming data in real-time. The goal is to keep things simple and take some of the burden away from existing SIEM and Analytics platforms by running various pipelines (combination of tasks that perform transformations and detections). PADAS comes with out-of-the-box integration examples (e.g. Winlogbeat, Splunk) and ready-to-implement rules pertinent to MITRE ATT&CK Framework. Any other custom rules can also be added (via PDL) without any dependency to existing SIEM or Analytics infrastructure. Set up in 5 minutes Review PADAS design and architectural considerations and quickly get started on your preferred platform. Getting started It's just Markdown Details on system requirements and how to install and run PADAS in production environments. Reference Made to measure This guide provides how to utilize PADAS Manager for configuring properties and PDL rules. Customization Open Source, MIT Integrate with data ingest pipelines (e.g. Winlogbeat) and downstream SIEM/Analytics platforms (e.g. Splunk) License","title":"PADAS Overview"},{"location":"admin-guide/","text":"Admin Guide JVM Settings Java 8 and Java 11 are supported in this version of PADAS. From a security perspective, we recommend the latest released patch version as older freely available versions have disclosed security vulnerabilities. For more information regarding Confluent Platform, please visit here . You need to separately install the correct version of Java before you start the installation process. JVM Heap Options can be set via PADAS_HEAP_OPTS environment variable. Default value is: -Xmx1G -Xms1G NOTE : When using systemctl to start the service, you'll need to edit padas.service file to make a change. Topic Properties Following Kafka topics are required for PADAS to operate properly. Upon initial access to Padas Manager, Topics View allows changing partition and replication counts. NOTE : For production, it is highly recommended to review Topic Configuration and tune settings for each Padas topic according to expected volume and performance requirements. {% include docs/props_topics.md %} Configuration Properties For any PADAS instance all configuration is read from $PADAS_HOME/etc/padas.properties file; and details regarding the properties settings can be found in padas.properties.spec , also available with any installation at $PADAS_HOME/etc/padas.properties.spec At a minimum, following settings are required for any PADAS instance: #################################### # SETTINGS FOR ALL INSTANCES #################################### #################################### # Required Settings # These settings must be defined for each instance regardless of its role. #################################### padas.instance.role=detect # Required - detect | transform | manager # Defines PADAS Component to run for this instance # Default: detect bootstrap.servers=localhost:9092 # Required - <string> # Kafka bootstrap server list with port information. Multiple servers can be separated by comma. # Default: localhost:9092 schema.registry.url=http://localhost:8081 # Required - <string> # Confluent Schema Registry URL # Default: http://localhost:8081 For Manager instance, license entry is required and the following settings are also applicable: #################################### # SETTINGS FOR MANAGER # Following settings are only used by manager instance #################################### #################################### # Required Settings #################################### padas.license= # Required - <string> # This setting is only used by 'manager' role. # License string should be a pipe '|' delimeted string containing # version, entitlement, startdate, enddate, quota, type, and signature # Default: none #################################### # Web Settings #################################### server.port=9000 # Optional - <integer> # Port number for Manager web interface # Default: 9000 server.ssl.enabled=true # Optional - <boolean> # Enable SSL for Manager web interface with the server.ssl.* configuration # Default: true server.ssl.key-alias=padas_ssl_cert # Optional - <string> # SSL Key alias for certificate # Default: padas_ssl_cert server.ssl.key-password=password # Optional - <string> # SSL Key password # Default: password server.ssl.key-store-password=password # Optional - <string> # SSL Key Store password # Default: password server.ssl.key-store=classpath:ssl-server.jks # Optional - <string> # SSL Key Store path, this should be the full path (e.g. /opt/padas/etc/certs/my.jks) # Default: reads from built-in classpath server.ssl.key-store-provider=SUN # Optional - <string> # SSL Key Store provider # Default: SUN server.ssl.key-store-type=JKS # Optional - <string> # SSL Key Store type # Default: padas_ssl_cert Logging PADAS utilizes Logback for logging application activity. By default, $PADAS_HOME/etc/logback.xml file is used; log files are created based on the following settings and can be changed according to your requirements. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <configuration> <!-- Stop output INFO at start --> <statusListener class= \"ch.qos.logback.core.status.NopStatusListener\" /> <property name= \"LOGS\" value= \"${PADAS_HOME}/logs\" /> <appender name= \"STDOUT\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %yellow(%d{yyyy-MM-dd HH:mm:ss.SSS}) %cyan(${HOSTNAME}) %magenta([%thread]) %highlight(%-5level) %logger{36}.%M - %msg%n </pattern> </encoder> </appender> <appender name= \"DISPLAY\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %highlight(%-5level) %msg%n </pattern> </encoder> </appender> <appender name= \"FILE-ROLLING\" class= \"ch.qos.logback.core.rolling.RollingFileAppender\" > <file> ${LOGS}/padas.log </file> <rollingPolicy class= \"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy\" > <fileNamePattern> ${LOGS}/padas.%d{yyyy-MM-dd}.%i.log </fileNamePattern> <!-- each archived file, size max 100MB --> <maxFileSize> 100MB </maxFileSize> <!-- total size of all archive files, if total size > 20GB, it will delete old archived file --> <totalSizeCap> 20GB </totalSizeCap> <!-- 60 days to keep --> <maxHistory> 60 </maxHistory> </rollingPolicy> <encoder> <pattern> %d{yyyy-MM-dd HH:mm:ss.SSS} ${HOSTNAME} [%thread] %-5level %logger{36}.%M - %msg%n </pattern> </encoder> </appender> <logger name= \"ch.qos.logback\" level= \"WARN\" /> <logger name= \"org.springframework\" level= \"WARN\" /> <logger name= \"org.apache\" level= \"WARN\" /> <logger name= \"io.confluent\" level= \"WARN\" /> <logger name= \"io.padas\" level= \"INFO\" > <!--<appender-ref ref=\"STDOUT\" />--> </logger> <logger name= \"io.padas.app.management.Manager\" level= \"INFO\" > <appender-ref ref= \"DISPLAY\" /> </logger> <logger name= \"io.padas.app.App\" level= \"INFO\" > <appender-ref ref= \"DISPLAY\" /> </logger> <root level= \"info\" > <appender-ref ref= \"FILE-ROLLING\" /> </root> </configuration> Integrate to External Systems It is possible to integrate any external system either as a Kafka Producer (source, generating and ingesting event data) or Kafka Consumer (sink, consuming padas_alerts topic for further analysis/alerting). Confluent Hub can be utilized to implement any specific source and/or sink connector for integration. Winlogbeat (Elastic Stack) Winlogbeat (OSS) can be utilized as a Kafka Producer to ingest Windows event data. You can find relevant example information below. Winlogbeat examples : - Sample Sysmon Config with Winlogbeat : This example sysmon configuration is based on Swift On Security sysmon config and it focuses on default high-quality event tracing while excluding any Winlogbeat generated activity from event logs. Winlogbeat configuration ( winlogbeat.yml ) : This is an example winlogbeat configuration that reads both Security and Sysmon event logs on the installed Windows system and sends events to relevant Kafka topics (i.e. winlogbeat-sysmon and winlogbeat-security ). PADAS configurations : Winlogbeat Sysmon and Security : This example properties file is for PADAS and can be uploaded via Properties view to quickly start transformations on Winlogbeat generated events as specified with the above examples. Out-of-the-box PADAS Rules : This JSON configuration contains MITRE ATT&CK relevant rules, which are tested and verified with the above example configurations. You can upload this file via Rules view to quickly get started. For any other input, it's recommended to review the applicable data model and PDL query of each rule for accuracy. Splunk Splunk can act as a Kafka Consumer for further analysis of Padas Alerts. Padas and Splunk integration can be accomplished seamlessly with Splunk Sink Connector and Technology Add-on for Padas . Splunk Sink Connector needs to be installed on Confluent Kafka and TA-Padas will need to be installed on Splunk Search Head(s). Please follow the instructions within the links on how to properly install. An example configuration for Splunk Sink Connector can be found here: splunk-sink-connector-example.json { \"name\" : \"SplunkSinkConnectorConnector_Padas\" , \"config\" : { \"connector.class\" : \"com.splunk.kafka.connect.SplunkSinkConnector\" , \"value.converter\" : \"io.confluent.connect.avro.AvroConverter\" , \"topics\" : \"padas_alerts\" , \"splunk.hec.token\" : \"e8de5f0e-97b1-4485-b416-9391cbf89392\" , \"splunk.hec.uri\" : \"https://splunk-server:8088\" , \"splunk.indexes\" : \"padas\" , \"splunk.sourcetypes\" : \"padas:alert\" , \"splunk.hec.ssl.validate.certs\" : \"false\" , \"value.converter.schema.registry.url\" : \"http://confluent-kafka-schema-registry-server:8081\" } } If the Splunk installation has MITRE ATT&CK App for Splunk , then any alert with MITRE ATT&CK annotations are automatically integrated also.","title":"Admin Guide"},{"location":"admin-guide/#admin-guide","text":"","title":"Admin Guide"},{"location":"admin-guide/#jvm-settings","text":"Java 8 and Java 11 are supported in this version of PADAS. From a security perspective, we recommend the latest released patch version as older freely available versions have disclosed security vulnerabilities. For more information regarding Confluent Platform, please visit here . You need to separately install the correct version of Java before you start the installation process. JVM Heap Options can be set via PADAS_HEAP_OPTS environment variable. Default value is: -Xmx1G -Xms1G NOTE : When using systemctl to start the service, you'll need to edit padas.service file to make a change.","title":"JVM Settings"},{"location":"admin-guide/#topic-properties","text":"Following Kafka topics are required for PADAS to operate properly. Upon initial access to Padas Manager, Topics View allows changing partition and replication counts. NOTE : For production, it is highly recommended to review Topic Configuration and tune settings for each Padas topic according to expected volume and performance requirements. {% include docs/props_topics.md %}","title":"Topic Properties"},{"location":"admin-guide/#configuration-properties","text":"For any PADAS instance all configuration is read from $PADAS_HOME/etc/padas.properties file; and details regarding the properties settings can be found in padas.properties.spec , also available with any installation at $PADAS_HOME/etc/padas.properties.spec At a minimum, following settings are required for any PADAS instance: #################################### # SETTINGS FOR ALL INSTANCES #################################### #################################### # Required Settings # These settings must be defined for each instance regardless of its role. #################################### padas.instance.role=detect # Required - detect | transform | manager # Defines PADAS Component to run for this instance # Default: detect bootstrap.servers=localhost:9092 # Required - <string> # Kafka bootstrap server list with port information. Multiple servers can be separated by comma. # Default: localhost:9092 schema.registry.url=http://localhost:8081 # Required - <string> # Confluent Schema Registry URL # Default: http://localhost:8081 For Manager instance, license entry is required and the following settings are also applicable: #################################### # SETTINGS FOR MANAGER # Following settings are only used by manager instance #################################### #################################### # Required Settings #################################### padas.license= # Required - <string> # This setting is only used by 'manager' role. # License string should be a pipe '|' delimeted string containing # version, entitlement, startdate, enddate, quota, type, and signature # Default: none #################################### # Web Settings #################################### server.port=9000 # Optional - <integer> # Port number for Manager web interface # Default: 9000 server.ssl.enabled=true # Optional - <boolean> # Enable SSL for Manager web interface with the server.ssl.* configuration # Default: true server.ssl.key-alias=padas_ssl_cert # Optional - <string> # SSL Key alias for certificate # Default: padas_ssl_cert server.ssl.key-password=password # Optional - <string> # SSL Key password # Default: password server.ssl.key-store-password=password # Optional - <string> # SSL Key Store password # Default: password server.ssl.key-store=classpath:ssl-server.jks # Optional - <string> # SSL Key Store path, this should be the full path (e.g. /opt/padas/etc/certs/my.jks) # Default: reads from built-in classpath server.ssl.key-store-provider=SUN # Optional - <string> # SSL Key Store provider # Default: SUN server.ssl.key-store-type=JKS # Optional - <string> # SSL Key Store type # Default: padas_ssl_cert","title":"Configuration Properties"},{"location":"admin-guide/#logging","text":"PADAS utilizes Logback for logging application activity. By default, $PADAS_HOME/etc/logback.xml file is used; log files are created based on the following settings and can be changed according to your requirements. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <configuration> <!-- Stop output INFO at start --> <statusListener class= \"ch.qos.logback.core.status.NopStatusListener\" /> <property name= \"LOGS\" value= \"${PADAS_HOME}/logs\" /> <appender name= \"STDOUT\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %yellow(%d{yyyy-MM-dd HH:mm:ss.SSS}) %cyan(${HOSTNAME}) %magenta([%thread]) %highlight(%-5level) %logger{36}.%M - %msg%n </pattern> </encoder> </appender> <appender name= \"DISPLAY\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %highlight(%-5level) %msg%n </pattern> </encoder> </appender> <appender name= \"FILE-ROLLING\" class= \"ch.qos.logback.core.rolling.RollingFileAppender\" > <file> ${LOGS}/padas.log </file> <rollingPolicy class= \"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy\" > <fileNamePattern> ${LOGS}/padas.%d{yyyy-MM-dd}.%i.log </fileNamePattern> <!-- each archived file, size max 100MB --> <maxFileSize> 100MB </maxFileSize> <!-- total size of all archive files, if total size > 20GB, it will delete old archived file --> <totalSizeCap> 20GB </totalSizeCap> <!-- 60 days to keep --> <maxHistory> 60 </maxHistory> </rollingPolicy> <encoder> <pattern> %d{yyyy-MM-dd HH:mm:ss.SSS} ${HOSTNAME} [%thread] %-5level %logger{36}.%M - %msg%n </pattern> </encoder> </appender> <logger name= \"ch.qos.logback\" level= \"WARN\" /> <logger name= \"org.springframework\" level= \"WARN\" /> <logger name= \"org.apache\" level= \"WARN\" /> <logger name= \"io.confluent\" level= \"WARN\" /> <logger name= \"io.padas\" level= \"INFO\" > <!--<appender-ref ref=\"STDOUT\" />--> </logger> <logger name= \"io.padas.app.management.Manager\" level= \"INFO\" > <appender-ref ref= \"DISPLAY\" /> </logger> <logger name= \"io.padas.app.App\" level= \"INFO\" > <appender-ref ref= \"DISPLAY\" /> </logger> <root level= \"info\" > <appender-ref ref= \"FILE-ROLLING\" /> </root> </configuration>","title":"Logging"},{"location":"admin-guide/#integrate-to-external-systems","text":"It is possible to integrate any external system either as a Kafka Producer (source, generating and ingesting event data) or Kafka Consumer (sink, consuming padas_alerts topic for further analysis/alerting). Confluent Hub can be utilized to implement any specific source and/or sink connector for integration.","title":"Integrate to External Systems"},{"location":"admin-guide/#winlogbeat-elastic-stack","text":"Winlogbeat (OSS) can be utilized as a Kafka Producer to ingest Windows event data. You can find relevant example information below. Winlogbeat examples : - Sample Sysmon Config with Winlogbeat : This example sysmon configuration is based on Swift On Security sysmon config and it focuses on default high-quality event tracing while excluding any Winlogbeat generated activity from event logs. Winlogbeat configuration ( winlogbeat.yml ) : This is an example winlogbeat configuration that reads both Security and Sysmon event logs on the installed Windows system and sends events to relevant Kafka topics (i.e. winlogbeat-sysmon and winlogbeat-security ). PADAS configurations : Winlogbeat Sysmon and Security : This example properties file is for PADAS and can be uploaded via Properties view to quickly start transformations on Winlogbeat generated events as specified with the above examples. Out-of-the-box PADAS Rules : This JSON configuration contains MITRE ATT&CK relevant rules, which are tested and verified with the above example configurations. You can upload this file via Rules view to quickly get started. For any other input, it's recommended to review the applicable data model and PDL query of each rule for accuracy.","title":"Winlogbeat (Elastic Stack)"},{"location":"admin-guide/#splunk","text":"Splunk can act as a Kafka Consumer for further analysis of Padas Alerts. Padas and Splunk integration can be accomplished seamlessly with Splunk Sink Connector and Technology Add-on for Padas . Splunk Sink Connector needs to be installed on Confluent Kafka and TA-Padas will need to be installed on Splunk Search Head(s). Please follow the instructions within the links on how to properly install. An example configuration for Splunk Sink Connector can be found here: splunk-sink-connector-example.json { \"name\" : \"SplunkSinkConnectorConnector_Padas\" , \"config\" : { \"connector.class\" : \"com.splunk.kafka.connect.SplunkSinkConnector\" , \"value.converter\" : \"io.confluent.connect.avro.AvroConverter\" , \"topics\" : \"padas_alerts\" , \"splunk.hec.token\" : \"e8de5f0e-97b1-4485-b416-9391cbf89392\" , \"splunk.hec.uri\" : \"https://splunk-server:8088\" , \"splunk.indexes\" : \"padas\" , \"splunk.sourcetypes\" : \"padas:alert\" , \"splunk.hec.ssl.validate.certs\" : \"false\" , \"value.converter.schema.registry.url\" : \"http://confluent-kafka-schema-registry-server:8081\" } } If the Splunk installation has MITRE ATT&CK App for Splunk , then any alert with MITRE ATT&CK annotations are automatically integrated also.","title":"Splunk"},{"location":"datamodel-reference/","text":"Datamodel Reference This is a reference guide for the Padas Datamodels, which can be used as a convention for easy integration with out-of-the-box PADAS rules as well as integrations with external systems . The sections below provide detailed field information regarding these datamodels. Endpoint Listening Port Datamodel Name: endpointListeningPort Field Name Data Type Description Example dest string The endpoint on which the port is listening. 10.10.1.1 destPort string Network port listening on the endpoint 80 processGuid string The globally unique identifier of the process assigned by the vendor_product. {f81d4fae-7dec-11d0-a765-00a0c91e6bf6} processId string The numeric identifier of the process assigned by the operating system. 456 src string The \"remote\" system connected to the listening port (if applicable). 192.168.1.10 srcPort string The \"remote\" port connected to the listening port (if applicable). 4567 state string The status of the listening port listening established transport string The network transport protocol associated with the listening port tcp udp user string The user account associated with the listening port. Endpoint Process Datamodel Name: endpointProcess Field Name Data Type Description Example action string The action taken by the endpoint access create terminate allowed blocked accessLevel string Permissions level at which the target process is accessed. 0x40 callTrace string The stack trace showing the context of a process open/access call. C:\\Windows\\SYSTEM32\\ntdll.dll+a5594|C:\\Windows\\system32\\KERNELBASE.dll+1e865 dest string The endpoint for which the process was spawned. 10.10.1.1 parentProcess string All of the arguments passed to the parent process upon execution. C:\\path\\example.exe /flag1 parentProcessExec string The executable name of the parent process example.exe parentProcessGuid string The globally unique identifier of the parent process assigned by the vendor_product. {f81d4fae-7dec-11d0-a765-00a0c91e6bf6} parentProcessId string The numeric identifier of the parent process assigned by the operating system. 837 parentProcessPath string The file path of the executable associated with this parent process. C:\\path\\to\\example.exe process string All of the arguments passed to the process upon execution. C:\\path\\example.exe /flag1 processCurrentDirectory string The absolute path to the current working directory of the process. c:\\windows\\system32\\ processExec string The executable name of the process example.exe processGuid string The globally unique identifier of the process assigned by the vendor_product. {f81d4fae-7dec-11d0-a765-00a0c91e6bf6} processHash string The digests of the contents of the file located at processPath by using md5, sha1, etc. 5eb63bbbe01eeed093cb22bb8f5acdc3 2aae6c35c94fcfb415dbe95f408b9ce91ee846ed processId string The numeric identifier of the process assigned by the operating system. 837 processIntegrityLevel string The Windows integrity level associated with the process. MUST be one of: low , medium , high , or system . medium processPath string The file path of the executable associated with this process. C:\\path\\to\\example.exe user string The user account that spawned the process. LOCALUSER userId string The unique identifier of the user account which spawned the process. For Windows, this is the security identifier, sid S-1-5-18 Endpoint Service Datamodel Name: endpointService Field Name Data Type Description Example action string The action performed on the service. create delete pause start stop dest string The endpoint on which the service is installed. 10.10.1.1 name string The name of the service. RpcSs parentProcessId string The numeric identifier of the parent process assigned by the operating system. 837 process string All of the arguments passed to the process upon execution. C:\\path\\example.exe /flag1 processExec string The executable name of the process example.exe processGuid string The globally unique identifier of the process assigned by the vendor_product. {f81d4fae-7dec-11d0-a765-00a0c91e6bf6} processHash string The digests of the contents of the file located at processPath by using md5, sha1, etc. 5eb63bbbe01eeed093cb22bb8f5acdc3 2aae6c35c94fcfb415dbe95f408b9ce91ee846ed processId string The numeric identifier of the process assigned by the operating system. 837 processPath string The file path of the executable associated with this process. C:\\path\\to\\example.exe startMode string The start mode for the service. disabled manual auto status string The status of the service. started stopped warning critical user string The user account that spawned the process. LOCALUSER userId string The unique identifier of the user account which spawned the process. For Windows, this is the security identifier, sid S-1-5-18 Endpoint File Datamodel Name: endpointFile Field Name Data Type Description Example action string The action performed on the resource. create delete modify read write dest string The endpoint on which the filesystem activity takes place. 10.10.1.1 fileCreationTime string The creation time of the file 05/14/2015 12:47:06 fileHash string The digests of the contents of the file located at filePath by using md5, sha1, etc. 5eb63bbbe01eeed093cb22bb8f5acdc3 2aae6c35c94fcfb415dbe95f408b9ce91ee846ed fileGroup string The group owner of the file admin fileGroupId string The group ID of the file 801 fileMode string The mode or permissions set of the file. 0644 (linux) or NTFS ACL fileName string The name of the file. MyWordDoc.docx fileOwner string The username of the owner of the file. adam fileOwnerId string The user ID or SID of the owner of the file. 501 filePath string The full path to the file on the file system. C:\\users\\fakeuser\\documents\\MyFile.docx parentProcessId string The numeric identifier of the parent process assigned by the operating system. 837 process string All of the arguments passed to the process upon execution. C:\\path\\example.exe /flag1 processExec string The executable name of the process example.exe processGuid string The globally unique identifier of the process assigned by the vendor_product. {f81d4fae-7dec-11d0-a765-00a0c91e6bf6} processId string The numeric identifier of the process assigned by the operating system. 837 processPath string The file path of the executable associated with this process. C:\\path\\to\\example.exe user string The user account that spawned the process. LOCALUSER userId string The unique identifier of the user account which spawned the process. For Windows, this is the security identifier, sid S-1-5-18 Endpoint Registry Datamodel Name: endpointRegistry Field Name Data Type Description Example action string The action performed on the resource. create delete modify read dest string The endpoint on which the port is listening. 10.10.1.1 process string All of the arguments passed to the process upon execution. C:\\path\\example.exe /flag1 processGuid string The globally unique identifier of the process assigned by the vendor_product. {f81d4fae-7dec-11d0-a765-00a0c91e6bf6} processId string The numeric identifier of the process assigned by the operating system. 456 registryHive string The logical group of keys, subkeys, and values in the registry. HKEY_CURRENT_USER HKEY_LOCAL_MACHINE registryKey string The registry key specified in the event. Similar to a folder in a traditional file system. HKLM\\SYSTEM\\CurrentControlSet\\services\\RpcSs registryValueName string The descriptive name for the data being stored in the key. InstalledVersion registryValueData string The contents of the value, typically a text string. %SystemRoot%\\system32\\svchost.exe -k rpcss registryValueType string The type of data being stored in the value. Types include binary data, 32 bit numbers, strings, etc. REG_SZ REG_MULTI_SZ REG_DWORD REG_BINARY REG_QWORD status string The outcome of the registry action. failure success user string The user account associated with the listening port.","title":"Datamodel Reference"},{"location":"datamodel-reference/#datamodel-reference","text":"This is a reference guide for the Padas Datamodels, which can be used as a convention for easy integration with out-of-the-box PADAS rules as well as integrations with external systems . The sections below provide detailed field information regarding these datamodels.","title":"Datamodel Reference"},{"location":"datamodel-reference/#endpoint-listening-port","text":"Datamodel Name: endpointListeningPort Field Name Data Type Description Example dest string The endpoint on which the port is listening. 10.10.1.1 destPort string Network port listening on the endpoint 80 processGuid string The globally unique identifier of the process assigned by the vendor_product. {f81d4fae-7dec-11d0-a765-00a0c91e6bf6} processId string The numeric identifier of the process assigned by the operating system. 456 src string The \"remote\" system connected to the listening port (if applicable). 192.168.1.10 srcPort string The \"remote\" port connected to the listening port (if applicable). 4567 state string The status of the listening port listening established transport string The network transport protocol associated with the listening port tcp udp user string The user account associated with the listening port.","title":"Endpoint Listening Port"},{"location":"datamodel-reference/#endpoint-process","text":"Datamodel Name: endpointProcess Field Name Data Type Description Example action string The action taken by the endpoint access create terminate allowed blocked accessLevel string Permissions level at which the target process is accessed. 0x40 callTrace string The stack trace showing the context of a process open/access call. C:\\Windows\\SYSTEM32\\ntdll.dll+a5594|C:\\Windows\\system32\\KERNELBASE.dll+1e865 dest string The endpoint for which the process was spawned. 10.10.1.1 parentProcess string All of the arguments passed to the parent process upon execution. C:\\path\\example.exe /flag1 parentProcessExec string The executable name of the parent process example.exe parentProcessGuid string The globally unique identifier of the parent process assigned by the vendor_product. {f81d4fae-7dec-11d0-a765-00a0c91e6bf6} parentProcessId string The numeric identifier of the parent process assigned by the operating system. 837 parentProcessPath string The file path of the executable associated with this parent process. C:\\path\\to\\example.exe process string All of the arguments passed to the process upon execution. C:\\path\\example.exe /flag1 processCurrentDirectory string The absolute path to the current working directory of the process. c:\\windows\\system32\\ processExec string The executable name of the process example.exe processGuid string The globally unique identifier of the process assigned by the vendor_product. {f81d4fae-7dec-11d0-a765-00a0c91e6bf6} processHash string The digests of the contents of the file located at processPath by using md5, sha1, etc. 5eb63bbbe01eeed093cb22bb8f5acdc3 2aae6c35c94fcfb415dbe95f408b9ce91ee846ed processId string The numeric identifier of the process assigned by the operating system. 837 processIntegrityLevel string The Windows integrity level associated with the process. MUST be one of: low , medium , high , or system . medium processPath string The file path of the executable associated with this process. C:\\path\\to\\example.exe user string The user account that spawned the process. LOCALUSER userId string The unique identifier of the user account which spawned the process. For Windows, this is the security identifier, sid S-1-5-18","title":"Endpoint Process"},{"location":"datamodel-reference/#endpoint-service","text":"Datamodel Name: endpointService Field Name Data Type Description Example action string The action performed on the service. create delete pause start stop dest string The endpoint on which the service is installed. 10.10.1.1 name string The name of the service. RpcSs parentProcessId string The numeric identifier of the parent process assigned by the operating system. 837 process string All of the arguments passed to the process upon execution. C:\\path\\example.exe /flag1 processExec string The executable name of the process example.exe processGuid string The globally unique identifier of the process assigned by the vendor_product. {f81d4fae-7dec-11d0-a765-00a0c91e6bf6} processHash string The digests of the contents of the file located at processPath by using md5, sha1, etc. 5eb63bbbe01eeed093cb22bb8f5acdc3 2aae6c35c94fcfb415dbe95f408b9ce91ee846ed processId string The numeric identifier of the process assigned by the operating system. 837 processPath string The file path of the executable associated with this process. C:\\path\\to\\example.exe startMode string The start mode for the service. disabled manual auto status string The status of the service. started stopped warning critical user string The user account that spawned the process. LOCALUSER userId string The unique identifier of the user account which spawned the process. For Windows, this is the security identifier, sid S-1-5-18","title":"Endpoint Service"},{"location":"datamodel-reference/#endpoint-file","text":"Datamodel Name: endpointFile Field Name Data Type Description Example action string The action performed on the resource. create delete modify read write dest string The endpoint on which the filesystem activity takes place. 10.10.1.1 fileCreationTime string The creation time of the file 05/14/2015 12:47:06 fileHash string The digests of the contents of the file located at filePath by using md5, sha1, etc. 5eb63bbbe01eeed093cb22bb8f5acdc3 2aae6c35c94fcfb415dbe95f408b9ce91ee846ed fileGroup string The group owner of the file admin fileGroupId string The group ID of the file 801 fileMode string The mode or permissions set of the file. 0644 (linux) or NTFS ACL fileName string The name of the file. MyWordDoc.docx fileOwner string The username of the owner of the file. adam fileOwnerId string The user ID or SID of the owner of the file. 501 filePath string The full path to the file on the file system. C:\\users\\fakeuser\\documents\\MyFile.docx parentProcessId string The numeric identifier of the parent process assigned by the operating system. 837 process string All of the arguments passed to the process upon execution. C:\\path\\example.exe /flag1 processExec string The executable name of the process example.exe processGuid string The globally unique identifier of the process assigned by the vendor_product. {f81d4fae-7dec-11d0-a765-00a0c91e6bf6} processId string The numeric identifier of the process assigned by the operating system. 837 processPath string The file path of the executable associated with this process. C:\\path\\to\\example.exe user string The user account that spawned the process. LOCALUSER userId string The unique identifier of the user account which spawned the process. For Windows, this is the security identifier, sid S-1-5-18","title":"Endpoint File"},{"location":"datamodel-reference/#endpoint-registry","text":"Datamodel Name: endpointRegistry Field Name Data Type Description Example action string The action performed on the resource. create delete modify read dest string The endpoint on which the port is listening. 10.10.1.1 process string All of the arguments passed to the process upon execution. C:\\path\\example.exe /flag1 processGuid string The globally unique identifier of the process assigned by the vendor_product. {f81d4fae-7dec-11d0-a765-00a0c91e6bf6} processId string The numeric identifier of the process assigned by the operating system. 456 registryHive string The logical group of keys, subkeys, and values in the registry. HKEY_CURRENT_USER HKEY_LOCAL_MACHINE registryKey string The registry key specified in the event. Similar to a folder in a traditional file system. HKLM\\SYSTEM\\CurrentControlSet\\services\\RpcSs registryValueName string The descriptive name for the data being stored in the key. InstalledVersion registryValueData string The contents of the value, typically a text string. %SystemRoot%\\system32\\svchost.exe -k rpcss registryValueType string The type of data being stored in the value. Types include binary data, 32 bit numbers, strings, etc. REG_SZ REG_MULTI_SZ REG_DWORD REG_BINARY REG_QWORD status string The outcome of the registry action. failure success user string The user account associated with the listening port.","title":"Endpoint Registry"},{"location":"get-started/","text":"Getting Started What is PADAS? PADAS is built on top of robust Confluent Kafka Platform and enables organizations to transform, analyze, and filter TBs of streaming data in real-time. The goal is to keep things simple and take some of the burden away from existing SIEM and Analytics platforms by running various pipelines (combination of tasks that perform transformations and detections). PADAS comes with out-of-the-box integration examples (e.g. Winlogbeat, Splunk) and ready-to-implement rules pertinent to MITRE ATT&CK Framework. Any other custom rules can also be added (via PDL) without any dependency to existing SIEM or Analytics infrastructure. Components Padas has 2 main components: 1. Manager UI : All configuration changes (CRUD - Create, Read, Update, Delete operations) can be performed through Manager web interface. This is an optional but recommended component to manage configurations through Engine API. 2. Engine : Reads configurations from existing Padas topics and runs assigned (based on group setting) and enabled topologies. Each topology reads from a single source topic, runs one or more pipeline(s), and writes the resulting outputs to one or more output topic(s). Each pipeline consists of one or more task(s) where each task can perform a filter, transform, enrichment, or detection (rules) function. Please see below for details on concepts. A Manager UI can be configured to connect to a single Engine component. Engine components can be scaled up or down as needed with group assignments to distribute work-load. Basic Concepts Let's take a closer look at Padas configuration and engine's processing concepts. At a high-level, Padas Engine reads an input topic, processes data (pipelines and tasks) and writes to one or more output topics. Topologies, Pipelines, Tasks Quick start This quick start guide assumes all components (Confluent Kafka and Padas) will be installed on the same machine. In production, it is recommended to separate out these components on different nodes/hosts. This quickstart consists of the following steps: 1. Step 1 : Download and define components 2. Step 2 : Start Manager 3. Step 3 : Start Detect Engine 4. Step 4 : Start Transform Engine 5. Step 5 : Generate Events 6. Step 6 : View Alerts Prerequisites Internet connectivity Supported Operating System A supported version of Java . Java 8 and Java 11 are supported in this version. Confluent Kafka must be installed and running (locally) as described in Quick Start for Confluent Platform . You should have at least the following services up and running. confluent local services status ... Kafka is [ UP ] Schema Registry is [ UP ] ZooKeeper is [ UP ] ... Overview of Quickstart Below diagram shows what will be accomplished with this quick start guide. Step 1 : Download and define components 1. Download the latest version (e.g. padas-{{ site.data.versions.latest_version }}.tgz ) 2. Use the tar command to decompress the archive file ```sh tar -xvzf padas-{{ site.data.versions.latest_version }}.tgz ``` Since we have everything on a single host, make a copy of the extracted folder for manager, transform engine, and detect engine cp -r padas padas-manager cp -r padas padas-transform mv padas padas-detect NOTE : Last renaming step is not necessary but gives a descriptive name to the folder's functionality. 4. Edit manager properties ( padas-manager/etc/padas.properties ) to make sure the padas.instance.role is set to manager and padas.license is set to the license you received. vi padas-manager/etc/padas.properties ... After editing, properties file ( padas-manager/etc/padas.properties ) entries should be: padas.instance.role = manager bootstrap.servers = localhost:9092 schema.registry.url = http://localhost:8081 padas.license = <LICENSE KEY SHOULD GO HERE> 5. Edit transform properties ( padas-transform/etc/padas.properties ) to make sure the padas.instance.role is set to transform . vi padas-transform/etc/padas.properties ... After editing, properties file ( padas-transform/etc/padas.properties ) entries should be: padas.instance.role = transform bootstrap.servers = localhost:9092 schema.registry.url = http://localhost:8081 6. From your current working directory, now you should have 3 PADAS folders, e.g. ls padas-detect padas-manager padas-transform Note that you don't have to make any configuration changes to padas-detect folder, as the default behavior is set to Detect Engine with localhost. At this stage, make sure you have Confluent Kafka running locally as mentioned in prerequisites. Step 2 : Start Manager 1. Start manager node on the console. The script will ask you to accept the license agreement (enter y ) and define an administrator user to login; enter the desired password to continue cd padas-manager/ {% include docs/padas_manager_start_console.md %} 2. Login : Go to http://localhost:9000 and login with the credentials used in previous step (e.g. admin) <img src=\"/assets/img/login_sample.png\" width=\"67%\"> Create Topics : Upon initial login, Manager will go to Topics menu in order to create necessary Kafka topics. Hit Save button to continue with defaults. Create a Rule : Go to Rules menu link in order to add a sample rule. Enter the following values for the required fields: Rule Name : Test Rule PDL Query : field1=\"value\" Datamodel List : testdm Other provided fields are optional but feel free to review and add/modify as needed. A list of rules for MITRE ATT&CK can be found here: padasRules.json Hit Save button to continue. You should be able to view the rule you specified, similar to the following screenshot. Add a Transformation : Go to Properties and first hit Edit , then select Add New Transformation . Expand \"Input Topic: 0\" and enter the following values for the required fields: Topic Name : testtopic Datamodel Name : testdm You should be able to view the newly added property (Input Topic: testtopic, similar to the following screenshot. Step 3 : Start Detect Engine 1. Start Detect Engine on the console (separate window, since Manager is running on the console as well). The script will ask you to accept the license agreement (enter y ). cd padas-detect/ Step 4 : Start Transform Engine 1. Before starting Transform Engine we must first create the specified input topic (i.e. testtopic ) in Kafka. You can do this from Confluent Control Center or from the console as shown below. ```sh kafka-topics --create --bootstrap-server localhost:9092 --topic testtopic --partitions 1 --replication-factor 1 Created topic testtopic. ``` Start Transform Engine on the console (separate window, since Manager and Detect Engine are running on the console as well). The script will ask you to accept the license agreement (enter y ). cd padas-transform/ Step 5 : Generate Sample Event Let's generate a sample event with a simple JSON message. Note that this JSON will match the PDL ( field1=\"value1\" ) specified above. echo '{\"field1\":\"value1\",\"field2\":\"value1\"}' | kafka-console-producer --bootstrap-server localhost:9092 --topic testtopic Step 6 : View Alerts Once the sample event is ingested, PADAS Detect Engine will run the rules for matching datamodels in real-time and populate padas_alerts topic with matching event and alert information. You can simply view this alert with the following command: kafka-avro-console-consumer --bootstrap-server localhost:9092 --topic padas_alerts --from-beginning | jq Output will be similar to the following. Note the use of jq above for pretty display of JSON data. { \"timestamp\" : \"2021-11-28T14:25:23.199+0300\" , \"name\" : \"Test Rule\" , \"description\" : \"\" , \"references\" : null , \"customAnnotations\" : null , \"mitreAnnotations\" : null , \"platforms\" : { \"string\" : \"\" }, \"domain\" : \"mitre_attack\" , \"analyticType\" : { \"string\" : \"\" }, \"severity\" : { \"string\" : \"\" }, \"datamodelReferences\" : null , \"events\" : [ { \"timestamp\" : \"2021-11-28T14:18:30.309+0300\" , \"datamodel\" : \"testdm\" , \"source\" : \"unknown\" , \"host\" : \"padas.local\" , \"src\" : null , \"dest\" : null , \"user\" : null , \"rawdata\" : \"{\\\"field1\\\":\\\"value1\\\",\\\"field2\\\":\\\"value1\\\"}\" , \"jsondata\" : \"{\\\"field1\\\":\\\"value1\\\",\\\"field2\\\":\\\"value1\\\"}\" } ] } Next Steps : - Install in production. - Utilize PADAS with out-of-the-box padasRules.json - Integrations with ingest pipelines ( Sample Sysmon Config with Winlogbeat ) and ready-to-use transformations ( Winlogbeat Sysmon and Security )","title":"Getting Started"},{"location":"get-started/#getting-started","text":"","title":"Getting Started"},{"location":"get-started/#what-is-padas","text":"PADAS is built on top of robust Confluent Kafka Platform and enables organizations to transform, analyze, and filter TBs of streaming data in real-time. The goal is to keep things simple and take some of the burden away from existing SIEM and Analytics platforms by running various pipelines (combination of tasks that perform transformations and detections). PADAS comes with out-of-the-box integration examples (e.g. Winlogbeat, Splunk) and ready-to-implement rules pertinent to MITRE ATT&CK Framework. Any other custom rules can also be added (via PDL) without any dependency to existing SIEM or Analytics infrastructure.","title":"What is PADAS?"},{"location":"get-started/#components","text":"Padas has 2 main components: 1. Manager UI : All configuration changes (CRUD - Create, Read, Update, Delete operations) can be performed through Manager web interface. This is an optional but recommended component to manage configurations through Engine API. 2. Engine : Reads configurations from existing Padas topics and runs assigned (based on group setting) and enabled topologies. Each topology reads from a single source topic, runs one or more pipeline(s), and writes the resulting outputs to one or more output topic(s). Each pipeline consists of one or more task(s) where each task can perform a filter, transform, enrichment, or detection (rules) function. Please see below for details on concepts. A Manager UI can be configured to connect to a single Engine component. Engine components can be scaled up or down as needed with group assignments to distribute work-load.","title":"Components"},{"location":"get-started/#basic-concepts","text":"Let's take a closer look at Padas configuration and engine's processing concepts. At a high-level, Padas Engine reads an input topic, processes data (pipelines and tasks) and writes to one or more output topics. Topologies, Pipelines, Tasks","title":"Basic Concepts"},{"location":"get-started/#_1","text":"","title":""},{"location":"get-started/#quick-start","text":"This quick start guide assumes all components (Confluent Kafka and Padas) will be installed on the same machine. In production, it is recommended to separate out these components on different nodes/hosts. This quickstart consists of the following steps: 1. Step 1 : Download and define components 2. Step 2 : Start Manager 3. Step 3 : Start Detect Engine 4. Step 4 : Start Transform Engine 5. Step 5 : Generate Events 6. Step 6 : View Alerts","title":"Quick start"},{"location":"get-started/#prerequisites","text":"Internet connectivity Supported Operating System A supported version of Java . Java 8 and Java 11 are supported in this version. Confluent Kafka must be installed and running (locally) as described in Quick Start for Confluent Platform . You should have at least the following services up and running. confluent local services status ... Kafka is [ UP ] Schema Registry is [ UP ] ZooKeeper is [ UP ] ...","title":"Prerequisites"},{"location":"get-started/#overview-of-quickstart","text":"Below diagram shows what will be accomplished with this quick start guide.","title":"Overview of Quickstart"},{"location":"get-started/#_2","text":"Step 1 : Download and define components 1. Download the latest version (e.g. padas-{{ site.data.versions.latest_version }}.tgz ) 2. Use the tar command to decompress the archive file ```sh tar -xvzf padas-{{ site.data.versions.latest_version }}.tgz ``` Since we have everything on a single host, make a copy of the extracted folder for manager, transform engine, and detect engine cp -r padas padas-manager cp -r padas padas-transform mv padas padas-detect NOTE : Last renaming step is not necessary but gives a descriptive name to the folder's functionality. 4. Edit manager properties ( padas-manager/etc/padas.properties ) to make sure the padas.instance.role is set to manager and padas.license is set to the license you received. vi padas-manager/etc/padas.properties ... After editing, properties file ( padas-manager/etc/padas.properties ) entries should be: padas.instance.role = manager bootstrap.servers = localhost:9092 schema.registry.url = http://localhost:8081 padas.license = <LICENSE KEY SHOULD GO HERE> 5. Edit transform properties ( padas-transform/etc/padas.properties ) to make sure the padas.instance.role is set to transform . vi padas-transform/etc/padas.properties ... After editing, properties file ( padas-transform/etc/padas.properties ) entries should be: padas.instance.role = transform bootstrap.servers = localhost:9092 schema.registry.url = http://localhost:8081 6. From your current working directory, now you should have 3 PADAS folders, e.g. ls padas-detect padas-manager padas-transform Note that you don't have to make any configuration changes to padas-detect folder, as the default behavior is set to Detect Engine with localhost. At this stage, make sure you have Confluent Kafka running locally as mentioned in prerequisites.","title":""},{"location":"get-started/#_3","text":"Step 2 : Start Manager 1. Start manager node on the console. The script will ask you to accept the license agreement (enter y ) and define an administrator user to login; enter the desired password to continue cd padas-manager/ {% include docs/padas_manager_start_console.md %} 2. Login : Go to http://localhost:9000 and login with the credentials used in previous step (e.g. admin) <img src=\"/assets/img/login_sample.png\" width=\"67%\"> Create Topics : Upon initial login, Manager will go to Topics menu in order to create necessary Kafka topics. Hit Save button to continue with defaults. Create a Rule : Go to Rules menu link in order to add a sample rule. Enter the following values for the required fields: Rule Name : Test Rule PDL Query : field1=\"value\" Datamodel List : testdm Other provided fields are optional but feel free to review and add/modify as needed. A list of rules for MITRE ATT&CK can be found here: padasRules.json Hit Save button to continue. You should be able to view the rule you specified, similar to the following screenshot. Add a Transformation : Go to Properties and first hit Edit , then select Add New Transformation . Expand \"Input Topic: 0\" and enter the following values for the required fields: Topic Name : testtopic Datamodel Name : testdm You should be able to view the newly added property (Input Topic: testtopic, similar to the following screenshot.","title":""},{"location":"get-started/#_4","text":"Step 3 : Start Detect Engine 1. Start Detect Engine on the console (separate window, since Manager is running on the console as well). The script will ask you to accept the license agreement (enter y ). cd padas-detect/","title":""},{"location":"get-started/#_5","text":"Step 4 : Start Transform Engine 1. Before starting Transform Engine we must first create the specified input topic (i.e. testtopic ) in Kafka. You can do this from Confluent Control Center or from the console as shown below. ```sh kafka-topics --create --bootstrap-server localhost:9092 --topic testtopic --partitions 1 --replication-factor 1 Created topic testtopic. ``` Start Transform Engine on the console (separate window, since Manager and Detect Engine are running on the console as well). The script will ask you to accept the license agreement (enter y ). cd padas-transform/","title":""},{"location":"get-started/#_6","text":"Step 5 : Generate Sample Event Let's generate a sample event with a simple JSON message. Note that this JSON will match the PDL ( field1=\"value1\" ) specified above. echo '{\"field1\":\"value1\",\"field2\":\"value1\"}' | kafka-console-producer --bootstrap-server localhost:9092 --topic testtopic","title":""},{"location":"get-started/#_7","text":"Step 6 : View Alerts Once the sample event is ingested, PADAS Detect Engine will run the rules for matching datamodels in real-time and populate padas_alerts topic with matching event and alert information. You can simply view this alert with the following command: kafka-avro-console-consumer --bootstrap-server localhost:9092 --topic padas_alerts --from-beginning | jq Output will be similar to the following. Note the use of jq above for pretty display of JSON data. { \"timestamp\" : \"2021-11-28T14:25:23.199+0300\" , \"name\" : \"Test Rule\" , \"description\" : \"\" , \"references\" : null , \"customAnnotations\" : null , \"mitreAnnotations\" : null , \"platforms\" : { \"string\" : \"\" }, \"domain\" : \"mitre_attack\" , \"analyticType\" : { \"string\" : \"\" }, \"severity\" : { \"string\" : \"\" }, \"datamodelReferences\" : null , \"events\" : [ { \"timestamp\" : \"2021-11-28T14:18:30.309+0300\" , \"datamodel\" : \"testdm\" , \"source\" : \"unknown\" , \"host\" : \"padas.local\" , \"src\" : null , \"dest\" : null , \"user\" : null , \"rawdata\" : \"{\\\"field1\\\":\\\"value1\\\",\\\"field2\\\":\\\"value1\\\"}\" , \"jsondata\" : \"{\\\"field1\\\":\\\"value1\\\",\\\"field2\\\":\\\"value1\\\"}\" } ] }","title":""},{"location":"get-started/#_8","text":"Next Steps : - Install in production. - Utilize PADAS with out-of-the-box padasRules.json - Integrations with ingest pipelines ( Sample Sysmon Config with Winlogbeat ) and ready-to-use transformations ( Winlogbeat Sysmon and Security )","title":""},{"location":"installation/","text":"Installation System Requirements Hardware The following machine recommendations are for installing individual PADAS components: Software {% include docs/sysreq_confluent.md %} Operating Systems PADAS supports the following operating systems. Java {% include docs/sysreq_java.md %} Manual Install Using TAR Archive Download the software Download the latest version here: padas-{{ site.data.versions.latest_version }}.tgz or via command line: curl -O https://www.padas.io/assets/downloads/padas- {{ site.data.versions.latest_version }} .tgz NOTE: You can verify the integrity by checking against its SHA512 checksum: padas-{{ site.data.versions.latest_version }}.tgz.sha512 Extract contents of the archive (default /opt is assumed for $PADAS_HOME ) cd /opt tar xvf padas- {{ site.data.versions.latest_version }} .tgz You should have these directories: {% include docs/padas_folders.md %} IMPORTANT NOTE : It is recommended to create a separate user to run Padas, other than root . In our examples, we use padas as both the user and group name. Following is an example on how to create such user: sudo useradd -d /opt/padas -U padas Register as a Service Run Padas to create a service file. (Note: following examples assume $PADAS_HOME to be /opt/padas directory) bin/padas set-service Systemd unit file has been created as '/opt/padas/libs/padas.service' Review the generated service file ( libs/padas.service ) and edit as necessary (e.g. user & group information, JVM memory options according to your system settings) [Unit] Description = PADAS - Alert Detection for Streaming Events Documentation = https://www.padas.io/docs After = network.target # [Service] Type = simple User = padas Group = padas ExecStart = java -Xmx1G -Xms1G -Dconfig.file=/opt/padas/etc/padas.properties -Dlogging.config=/opt/padas/etc/logback.xml -jar /opt/padas/libs/padas-{{ site.data.versions.latest_version }}.jar TimeoutStopSec = 180 Restart = no # [Install] WantedBy = multi-user.target Copy the service file under system sudo cp /opt/padas/libs/padas.service /etc/systemd/system/ Reload systemd process sudo systemctl daemon-reload Start For the First Time IMPORTANT NOTE : You need a running Kafka environment (Broker(s) and Schema Registry) and a PADAS license key. Request a license key if you don't have one. Manager Edit etc/padas.properties file to reflect your environment and enter the license key. Note that padas.instance.role MUST be manager . padas.instance.role = manager bootstrap.servers = localhost:9092 schema.registry.url = http://localhost:8081 padas.license = <ENTER YOUR LICENSE KEY HERE> Use command-line interface (CLI) to start PADAS: cd $PADAS_HOME /bin ./padas start PADAS displays the license agreement and prompts you to accept in order to continue. Create admin username. This is the user that you log into PADAS Manager with. Please enter an administrator username? [ admin ] : Create the password for the user that you just created. Password must contain at least 8 total printable ASCII characters. Please enter a new password: Please repeat the password: Successfully saved password. By default, Manager web interface starts on tcp/9000 port. Open a browser and access PADAS Manager (e.g. http://localhost:9000). For any custom configurations please go to Admin Guide Once logged in, unless the topics are created separately, Manager will prompt you to create the required Kafka topics. IMPORTANT NOTE : Number of partitions for each topic needs to be determined based on the expected event data load, performance requirements and your Kafka cluster setup. Please consult your Kafka administrator or PADAS representative for assistance. Once set, number of partitions can not be changed (the topic needs to be deleted and re-created). Go to Rules menu link and click Edit button in order to add rules. You can upload our out-of-the-box MITRE ATT&CK compatible rules, padasRules.json , that work with Winlogbeat eventlog from winlogbeat-sysmon and winlogbeat-security datamodels in padas_events 9. Go to Properties menu link and click Edit button in order to add properties. You can upload out-of-the-box transformations for Winlogbeat, padas_transformation.properties , which are configured for getting winlogbeat-sysmon and winlogbeat-security topics transformed into padas_events Detect Engine Edit etc/padas.properties file to reflect your environment. Note that padas.instance.role MUST be detect (default setting). padas.instance.role = detect bootstrap.servers = localhost:9092 schema.registry.url = http://localhost:8081 Use command-line interface (CLI) to start PADAS: cd $PADAS_HOME /bin ./padas start PADAS displays the license agreement and prompts you to accept in order to continue. Transform Engine Edit etc/padas.properties file to reflect your environment. Note that padas.instance.role MUST be transform . padas.instance.role = detect bootstrap.servers = localhost:9092 schema.registry.url = http://localhost:8081 Use command-line interface (CLI) to start PADAS: cd $PADAS_HOME /bin ./padas start PADAS displays the license agreement and prompts you to accept in order to continue. PADAS Command Line Interface A wrapper script is provided to manage PADAS service: $PADAS_HOME/bin/padas Example outputs when components are started for the first time. Manager: Detect Engine: Transform Engine: Uninstall Remove Padas directory. For example: rm -rf /opt/padas Docker TBD","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#system-requirements","text":"","title":"System Requirements"},{"location":"installation/#hardware","text":"The following machine recommendations are for installing individual PADAS components:","title":"Hardware"},{"location":"installation/#software","text":"{% include docs/sysreq_confluent.md %}","title":"Software"},{"location":"installation/#operating-systems","text":"PADAS supports the following operating systems.","title":"Operating Systems"},{"location":"installation/#java","text":"{% include docs/sysreq_java.md %}","title":"Java"},{"location":"installation/#manual-install-using-tar-archive","text":"","title":"Manual Install Using TAR Archive"},{"location":"installation/#download-the-software","text":"Download the latest version here: padas-{{ site.data.versions.latest_version }}.tgz or via command line: curl -O https://www.padas.io/assets/downloads/padas- {{ site.data.versions.latest_version }} .tgz NOTE: You can verify the integrity by checking against its SHA512 checksum: padas-{{ site.data.versions.latest_version }}.tgz.sha512 Extract contents of the archive (default /opt is assumed for $PADAS_HOME ) cd /opt tar xvf padas- {{ site.data.versions.latest_version }} .tgz You should have these directories: {% include docs/padas_folders.md %} IMPORTANT NOTE : It is recommended to create a separate user to run Padas, other than root . In our examples, we use padas as both the user and group name. Following is an example on how to create such user: sudo useradd -d /opt/padas -U padas","title":"Download the software"},{"location":"installation/#register-as-a-service","text":"Run Padas to create a service file. (Note: following examples assume $PADAS_HOME to be /opt/padas directory) bin/padas set-service Systemd unit file has been created as '/opt/padas/libs/padas.service' Review the generated service file ( libs/padas.service ) and edit as necessary (e.g. user & group information, JVM memory options according to your system settings) [Unit] Description = PADAS - Alert Detection for Streaming Events Documentation = https://www.padas.io/docs After = network.target # [Service] Type = simple User = padas Group = padas ExecStart = java -Xmx1G -Xms1G -Dconfig.file=/opt/padas/etc/padas.properties -Dlogging.config=/opt/padas/etc/logback.xml -jar /opt/padas/libs/padas-{{ site.data.versions.latest_version }}.jar TimeoutStopSec = 180 Restart = no # [Install] WantedBy = multi-user.target Copy the service file under system sudo cp /opt/padas/libs/padas.service /etc/systemd/system/ Reload systemd process sudo systemctl daemon-reload","title":"Register as a Service"},{"location":"installation/#start-for-the-first-time","text":"IMPORTANT NOTE : You need a running Kafka environment (Broker(s) and Schema Registry) and a PADAS license key. Request a license key if you don't have one.","title":"Start For the First Time"},{"location":"installation/#manager","text":"Edit etc/padas.properties file to reflect your environment and enter the license key. Note that padas.instance.role MUST be manager . padas.instance.role = manager bootstrap.servers = localhost:9092 schema.registry.url = http://localhost:8081 padas.license = <ENTER YOUR LICENSE KEY HERE> Use command-line interface (CLI) to start PADAS: cd $PADAS_HOME /bin ./padas start PADAS displays the license agreement and prompts you to accept in order to continue. Create admin username. This is the user that you log into PADAS Manager with. Please enter an administrator username? [ admin ] : Create the password for the user that you just created. Password must contain at least 8 total printable ASCII characters. Please enter a new password: Please repeat the password: Successfully saved password. By default, Manager web interface starts on tcp/9000 port. Open a browser and access PADAS Manager (e.g. http://localhost:9000). For any custom configurations please go to Admin Guide Once logged in, unless the topics are created separately, Manager will prompt you to create the required Kafka topics. IMPORTANT NOTE : Number of partitions for each topic needs to be determined based on the expected event data load, performance requirements and your Kafka cluster setup. Please consult your Kafka administrator or PADAS representative for assistance. Once set, number of partitions can not be changed (the topic needs to be deleted and re-created). Go to Rules menu link and click Edit button in order to add rules. You can upload our out-of-the-box MITRE ATT&CK compatible rules, padasRules.json , that work with Winlogbeat eventlog from winlogbeat-sysmon and winlogbeat-security datamodels in padas_events 9. Go to Properties menu link and click Edit button in order to add properties. You can upload out-of-the-box transformations for Winlogbeat, padas_transformation.properties , which are configured for getting winlogbeat-sysmon and winlogbeat-security topics transformed into padas_events","title":"Manager"},{"location":"installation/#detect-engine","text":"Edit etc/padas.properties file to reflect your environment. Note that padas.instance.role MUST be detect (default setting). padas.instance.role = detect bootstrap.servers = localhost:9092 schema.registry.url = http://localhost:8081 Use command-line interface (CLI) to start PADAS: cd $PADAS_HOME /bin ./padas start PADAS displays the license agreement and prompts you to accept in order to continue.","title":"Detect Engine"},{"location":"installation/#transform-engine","text":"Edit etc/padas.properties file to reflect your environment. Note that padas.instance.role MUST be transform . padas.instance.role = detect bootstrap.servers = localhost:9092 schema.registry.url = http://localhost:8081 Use command-line interface (CLI) to start PADAS: cd $PADAS_HOME /bin ./padas start PADAS displays the license agreement and prompts you to accept in order to continue.","title":"Transform Engine"},{"location":"installation/#padas-command-line-interface","text":"A wrapper script is provided to manage PADAS service: $PADAS_HOME/bin/padas Example outputs when components are started for the first time. Manager: Detect Engine: Transform Engine:","title":"PADAS Command Line Interface"},{"location":"installation/#uninstall","text":"Remove Padas directory. For example: rm -rf /opt/padas","title":"Uninstall"},{"location":"installation/#docker","text":"TBD","title":"Docker"},{"location":"pdl-reference/","text":"PDL Reference This is a reference guide for the Padas Domain Language (PDL). In this manual you will find explanation of PDL syntax, descriptions, and examples. In order to understand how PADAS works, please review Getting Started . PDL Syntax The following sections desribe the syntax used for Padas Domain Language (PDL) queries. PDL performs operations on a single event ( padas_events topic, jsondata field) and simply compares to the query, then returns a boolean response to indicate a match or mismatch. PDL syntax requires fields to be available in JSON object that it compares against. Examples Event jsondata value PDL Query Expected Result { \"field1\":{ \"subfield1\":\"subvalue1\", \"subfield2\":\"sub value2\" }, \"field2\":\"value2\", \"field3\":123 } field1.subfield2 ?= \"value2\" true { \"field1\":\"value1\", \"field2\":\"value2 text2 value2\", \"field3\":123, \"field4\":\"value4\", \"field_5\":5, \"field-6\":6, \"field:7\":7 } field1=\"va*e1\" true { \"field1\":{ \"subfield1\":\"subvalue1\", \"subfield2\":\"sub value2\" }, \"field2\":\"value2\", \"field3\":123 } (field1.subfield2 = \"value2\" AND field3=123) false Supported Operators PDL supports the following operators and keywords when comparing events to the query. Below table provides examples based on the following JSON value: { \"field1\":\"value1\", \"field2\":\"value2 text2 value2\", \"field3\":123 } Operator/Keyword Description Example ( true ) NOT NOT, negates the result NOT (field1 = \"valueXXX\") AND AND, expects both sides to be true field1=\"value1\" AND field3=123 OR OR, expects at least one side to be true field1=\"xyz\" OR field3=123 = Equals, returns true if the value is an exact match. A single wildcard * is also accepted for string values. field1=\"value1\" field1=\"val*\" != Not Equals, returns true if the value does not match field3 != 456 ?= Contains, checks whether the string value contains the query field2 ?= \"text2\" > Greater than, returns true if query comparison value is greater than event field value field3 > 100 < Less than, returns true if query comparison value is less than event field value field3 < 200 >= Greater than or equals, returns true if query comparison value is greater than or equals to the event field value field3 >= 123 <= Less than or equals, returns true if query comparison value is less than or equals to the event field value field3 < = 123 Supported JSON Values PDL comparisons work on String and Integer JSON values. String comparisons MUST be defined in quotes \" within PDL query definition. Examples: PDL query with field1=\"123\" will compare \"123\" as a String JSON value. PDL query with field2=123 will compare 123 as an Integer JSON value. Wildcard Support PDL supports a single wildcard * with Equals operator ( = ) for String JSON values. Following are valid PDL query examples with wildcard usage: field1=\"val*1\" field1=\"val*\" field1=\"*ue1\" Grouped arguments Sometimes the syntax must display arguments as a group to show that the set of arguments are used together. Parenthesis ( ) are used to group arguments. For example in this syntax: (field1=\"val1\" OR field2=123) AND field3=\"value3\" The grouped argument is (field1=\"val1\" OR field2=123) and its results are evaluated as a whole.","title":"PDL Reference"},{"location":"pdl-reference/#pdl-reference","text":"This is a reference guide for the Padas Domain Language (PDL). In this manual you will find explanation of PDL syntax, descriptions, and examples. In order to understand how PADAS works, please review Getting Started .","title":"PDL Reference"},{"location":"pdl-reference/#pdl-syntax","text":"The following sections desribe the syntax used for Padas Domain Language (PDL) queries. PDL performs operations on a single event ( padas_events topic, jsondata field) and simply compares to the query, then returns a boolean response to indicate a match or mismatch. PDL syntax requires fields to be available in JSON object that it compares against.","title":"PDL Syntax"},{"location":"pdl-reference/#examples","text":"Event jsondata value PDL Query Expected Result { \"field1\":{ \"subfield1\":\"subvalue1\", \"subfield2\":\"sub value2\" }, \"field2\":\"value2\", \"field3\":123 } field1.subfield2 ?= \"value2\" true { \"field1\":\"value1\", \"field2\":\"value2 text2 value2\", \"field3\":123, \"field4\":\"value4\", \"field_5\":5, \"field-6\":6, \"field:7\":7 } field1=\"va*e1\" true { \"field1\":{ \"subfield1\":\"subvalue1\", \"subfield2\":\"sub value2\" }, \"field2\":\"value2\", \"field3\":123 } (field1.subfield2 = \"value2\" AND field3=123) false","title":"Examples"},{"location":"pdl-reference/#supported-operators","text":"PDL supports the following operators and keywords when comparing events to the query. Below table provides examples based on the following JSON value: { \"field1\":\"value1\", \"field2\":\"value2 text2 value2\", \"field3\":123 } Operator/Keyword Description Example ( true ) NOT NOT, negates the result NOT (field1 = \"valueXXX\") AND AND, expects both sides to be true field1=\"value1\" AND field3=123 OR OR, expects at least one side to be true field1=\"xyz\" OR field3=123 = Equals, returns true if the value is an exact match. A single wildcard * is also accepted for string values. field1=\"value1\" field1=\"val*\" != Not Equals, returns true if the value does not match field3 != 456 ?= Contains, checks whether the string value contains the query field2 ?= \"text2\" > Greater than, returns true if query comparison value is greater than event field value field3 > 100 < Less than, returns true if query comparison value is less than event field value field3 < 200 >= Greater than or equals, returns true if query comparison value is greater than or equals to the event field value field3 >= 123 <= Less than or equals, returns true if query comparison value is less than or equals to the event field value field3 < = 123","title":"Supported Operators"},{"location":"pdl-reference/#supported-json-values","text":"PDL comparisons work on String and Integer JSON values. String comparisons MUST be defined in quotes \" within PDL query definition. Examples: PDL query with field1=\"123\" will compare \"123\" as a String JSON value. PDL query with field2=123 will compare 123 as an Integer JSON value.","title":"Supported JSON Values"},{"location":"pdl-reference/#wildcard-support","text":"PDL supports a single wildcard * with Equals operator ( = ) for String JSON values. Following are valid PDL query examples with wildcard usage: field1=\"val*1\" field1=\"val*\" field1=\"*ue1\"","title":"Wildcard Support"},{"location":"pdl-reference/#grouped-arguments","text":"Sometimes the syntax must display arguments as a group to show that the set of arguments are used together. Parenthesis ( ) are used to group arguments. For example in this syntax: (field1=\"val1\" OR field2=123) AND field3=\"value3\" The grouped argument is (field1=\"val1\" OR field2=123) and its results are evaluated as a whole.","title":"Grouped arguments"},{"location":"release-notes/","text":"Version 0.0.1 What's New? Feature Description {% for item in site.data.release_notes_100.features %} {{ item.feature }} {{ item.description }} {% endfor %} Known Issues Date Filed Issue Number Description {% for item in site.data.release_notes_100.known_issues %} {{ item.date }} {{ item.number }} {{ item.description }} {% endfor %} Fixed Issues Date Fixed Issue Number Description {% for item in site.data.release_notes_100.fixed_issues %} {{ item.date }} {{ item.number }} {{ item.description }} {% endfor %}","title":"Release Notes"},{"location":"release-notes/#_1","text":"","title":""},{"location":"release-notes/#version-001","text":"","title":"Version 0.0.1"},{"location":"release-notes/#whats-new","text":"Feature Description {% for item in site.data.release_notes_100.features %} {{ item.feature }} {{ item.description }} {% endfor %}","title":"What's New?"},{"location":"release-notes/#known-issues","text":"Date Filed Issue Number Description {% for item in site.data.release_notes_100.known_issues %} {{ item.date }} {{ item.number }} {{ item.description }} {% endfor %}","title":"Known Issues"},{"location":"release-notes/#fixed-issues","text":"Date Fixed Issue Number Description {% for item in site.data.release_notes_100.fixed_issues %} {{ item.date }} {{ item.number }} {{ item.description }} {% endfor %}","title":"Fixed Issues"},{"location":"user-guide/","text":"User Guide Account Settings You can view and edit current user's account settings (Display Name, Email address and password) via this view. About Overview Overview provides information regarding license information and registered nodes. Note that licensing quota is based on number of Detect nodes with an expiration date. Registered Node Information table provides details on actively running PADAS instances (other than this manager). Topics Upon initial login, PADAS Manager checks whether all required topics are created and available. If any one of the required topics is missing, you'll be redirected to Topics view in order to view and update existing settings. This is a simple interface to create required Kafka topics through PADAS Manager interface. Important Note : Number of partitions can NOT be changed/updated once a topic is created. This value depends on your data volume and scalability requirements. If you need to change/update this value for any reason, the topic will need to be deleted and created again with new values. For more information regarding topics, please refer to Topic Properties If you need more control over topic creation, please consult your Kafka/PADAS administrator; you can also refer to Confluent Documentation . Properties Properties view provides configuration entries for Detect and Transform Engine components. You can click Edit button to enter in edit mode and make changes. Following table provides information on the form fields. NOTE : You can upload (click Upload Properties from File button) and/or download (click Download Properties button) properties as a file. A sample properties file for Winlogbeat transformations can be found here: Winlogbeat Sysmon and Security NOTE : You can click Add New Transformation button to add new input topics for analysis. The input topic must exist prior to starting PADAS Transform Engine. NOTE : After any configuration changes, you will need to restart the corresponding component(s) (i.e. Detect and/or Transform Engine(s)). PADAS instances read and load the configuration upon starting. {% include docs/props_detect.md %} {% include docs/props_transform.md %} Properties View Sample Rules Rules view provides configuration entries for Detect Engine rules that are applicable to various data models (as specified in transformations or padas_events topic). Relevant schema for PADAS topics can be found here . NOTE : You can upload (click Upload Rules from File button) and/or download (click Download Rules button) rules as a JSON file. An out-of-the-box JSON rule file is provided for Winlogbeat according to MITRE ATT&CK framework and can be found here: padasRules.json NOTE : You can click Add New Rule button to add new detection rule. NOTE : Any change in detection rules is effective immediately (updates padas_rules topic) and does NOT require any restart/refresh. {% include docs/props_rules.md %} Rules View Sample","title":"User Guide"},{"location":"user-guide/#user-guide","text":"","title":"User Guide"},{"location":"user-guide/#account-settings","text":"You can view and edit current user's account settings (Display Name, Email address and password) via this view.","title":"Account Settings"},{"location":"user-guide/#about-overview","text":"Overview provides information regarding license information and registered nodes. Note that licensing quota is based on number of Detect nodes with an expiration date. Registered Node Information table provides details on actively running PADAS instances (other than this manager).","title":"About Overview"},{"location":"user-guide/#topics","text":"Upon initial login, PADAS Manager checks whether all required topics are created and available. If any one of the required topics is missing, you'll be redirected to Topics view in order to view and update existing settings. This is a simple interface to create required Kafka topics through PADAS Manager interface. Important Note : Number of partitions can NOT be changed/updated once a topic is created. This value depends on your data volume and scalability requirements. If you need to change/update this value for any reason, the topic will need to be deleted and created again with new values. For more information regarding topics, please refer to Topic Properties If you need more control over topic creation, please consult your Kafka/PADAS administrator; you can also refer to Confluent Documentation .","title":"Topics"},{"location":"user-guide/#properties","text":"Properties view provides configuration entries for Detect and Transform Engine components. You can click Edit button to enter in edit mode and make changes. Following table provides information on the form fields. NOTE : You can upload (click Upload Properties from File button) and/or download (click Download Properties button) properties as a file. A sample properties file for Winlogbeat transformations can be found here: Winlogbeat Sysmon and Security NOTE : You can click Add New Transformation button to add new input topics for analysis. The input topic must exist prior to starting PADAS Transform Engine. NOTE : After any configuration changes, you will need to restart the corresponding component(s) (i.e. Detect and/or Transform Engine(s)). PADAS instances read and load the configuration upon starting. {% include docs/props_detect.md %} {% include docs/props_transform.md %} Properties View Sample","title":"Properties"},{"location":"user-guide/#rules","text":"Rules view provides configuration entries for Detect Engine rules that are applicable to various data models (as specified in transformations or padas_events topic). Relevant schema for PADAS topics can be found here . NOTE : You can upload (click Upload Rules from File button) and/or download (click Download Rules button) rules as a JSON file. An out-of-the-box JSON rule file is provided for Winlogbeat according to MITRE ATT&CK framework and can be found here: padasRules.json NOTE : You can click Add New Rule button to add new detection rule. NOTE : Any change in detection rules is effective immediately (updates padas_rules topic) and does NOT require any restart/refresh. {% include docs/props_rules.md %} Rules View Sample","title":"Rules"}]}